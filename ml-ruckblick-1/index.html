<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>ML-Rückblick 1 - Machine Learning - Karlsruhe</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="//ml-ka.de/ml-ruckblick-1/">

        <meta name="author" content="Martin Thoma" />
        <meta name="keywords" content="Allgemein" />
        <meta name="description" content="Der ML-Rückblick gibt einen kurzen Überblick darüber, was seit dem letzen Rückblick in der Welt des maschinellen Lernens passiert ist." />

        <meta property="og:site_name" content="Machine Learning - Karlsruhe" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="ML-Rückblick 1"/>
        <meta property="og:url" content="//ml-ka.de/ml-ruckblick-1/"/>
        <meta property="og:description" content="Der ML-Rückblick gibt einen kurzen Überblick darüber, was seit dem letzen Rückblick in der Welt des maschinellen Lernens passiert ist."/>
        <meta property="article:published_time" content="2016-02-07" />
            <meta property="article:section" content="Allgemein" />
            <meta property="article:tag" content="Allgemein" />
            <meta property="article:author" content="Martin Thoma" />


    <!-- Bootstrap -->
        <link rel="stylesheet" href="//ml-ka.de/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="//ml-ka.de/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="//ml-ka.de/theme/css/pygments/native.css" rel="stylesheet">
    <link rel="stylesheet" href="//ml-ka.de/theme/css/style.css" type="text/css"/>

        <link href="//ml-ka.de/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Machine Learning - Karlsruhe ATOM Feed"/>



        <link href="//ml-ka.de/feeds/allgemein.atom.xml" type="application/atom+xml" rel="alternate"
              title="Machine Learning - Karlsruhe Allgemein ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="//ml-ka.de/" class="navbar-brand">
Machine Learning - Karlsruhe            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                        <li class="active">
                            <a href="//ml-ka.de/category/allgemein/">Allgemein</a>
                        </li>
                        <li >
                            <a href="//ml-ka.de/category/kit/">KIT</a>
                        </li>
                        <li >
                            <a href="//ml-ka.de/category/projekte/">Projekte</a>
                        </li>
                        <li >
                            <a href="//ml-ka.de/category/veranstaltungen/">Veranstaltungen</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><a href="//ml-ka.de/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="//ml-ka.de/ml-ruckblick-1/"
                       rel="bookmark"
                       title="Permalink to ML-Rückblick 1">
                        ML-Rückblick 1
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2016-02-07T12:00:00+01:00"> So 07 Februar 2016</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="//ml-ka.de/tag/allgemein/">Allgemein</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>Der ML-R&uuml;ckblick gibt einen kurzen &Uuml;berblick dar&uuml;ber, was seit dem letzen
R&uuml;ckblick in der Welt des maschinellen Lernens passiert ist.</p>
<h2 id="new-developments">New Developments</h2>
<!-- Trends -->
<h3 id="kogsys-demo">KogSys Demo</h3>
<p>Auf <a href="https://phiresky.github.io/neural-network-demo/">phiresky.github.io/neural-network-demo</a>
k&ouml;nnt ihr euch schnell mal selbst kleine Netzwerke und Datens&auml;tze
zusammenklicken. Dann k&ouml;nnt ihr beobachten, wie sich die Klassifikationsgrenzen
&auml;ndern.</p>
<figure style="display:table;margin: 0 auto 0.55em;">
<a href="//ml-ka.de/images/neural-network-kogsys-demo.png"><img align="middle" class="img-responsive" src="//ml-ka.de/images/neural-network-kogsys-demo.png" width="512"/></a>
<figcaption style="display:table-caption;caption-side:bottom">Interaktive Demo eines neuronalen Netzwerks</figcaption>
</figure>
<h3 id="howhot">HowHot</h3>
<p><a href="https://howhot.io/">howhot.io</a> ist eine Website, auf welcher man Fotos
hochladen kann. Das Programm findet dann ein Gesicht, kategorisiert in
"m&auml;nnlich" oder "weiblich", sch&auml;tzt das Alter und die Attraktivit&auml;t. Es gab
ein paar lustige Ergebnisse (siehe <a href="https://www.reddit.com/r/howhot/">Reddit</a>
sowie <a href="https://github.com/MartinThoma/seminar-art-in-machine-learning/tree/master/figures/eth-faces">ein paar weitere Bilder</a>).</p>
<h3 id="weitere">Weitere</h3>
<ul>
<li><a href="http://memorability.csail.mit.edu/">Large-scale Image Memorability</a></li>
</ul>
<h2 id="publications_1">Publications</h2>
<!-- e.g. arXiv -->
<h3 id="alphago">AlphaGo</h3>
<figure style="display:table;float:right">
<img align="middle" class="img-responsive" src="//ml-ka.de/images/go-game.png" style="float:right;" width="256">
<figcaption style="display:table-caption;caption-side:bottom">Go ist ein Brettspiel f&uuml;r zwei Spieler. Jeder Spieler hat jeweils nur einen Typ von Stein. Pro Zug darf ein Stein auf das 19&times;19 Feld gelegt werden.<br/>
Bildquelle: <a href="https://commons.wikimedia.org/wiki/File:Go_Regeln_3.png">Wikipedia Commons</a></figcaption>
</img></figure>
<p>Google hat eine Go-Engine namens AlphaGo entworfen. Diese soll den europ&auml;ischen
Go-Meister besiegt haben. Bald soll sie gegen den Go-Weltmeister antreten.</p>
<p>Erstaunlich ist, dass man die Go-Engine von Facebook (<a href="http://www.technologyreview.com/view/544181/how-facebooks-ai-researchers-built-a-game-changing-go-engine/?utm_campaign=socialsync&amp;utm_medium=social-post&amp;utm_source=facebook">Link</a>) nicht mal erw&auml;hnt.</p>
<p>Quellen und Materialien:</p>
<ul>
<li><a href="http://www.technologyreview.com/news/546066/googles-ai-masters-the-game-of-go-a-decade-earlier-than-expected/">technologyreview.com</a></li>
<li><a href="https://googleblog.blogspot.de/2016/01/alphago-machine-learning-game-go.html">Google Blog Artikel</a></li>
<li><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">Nature Artikel</a></li>
<li><a href="https://www.youtube.com/watch?v=g-dKXOlsf98">Nature Video</a></li>
<li>Paper: <a href="https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf">Mastering the Game of Go with Deep Neural Networks and Tree Search</a></li>
</ul>
<h3 id="deep-residual-networks">Deep Residual Networks</h3>
<p>Microsoft hat mit einem besonders tiefen neuronalen Netzwerk die Microsoft Common Objects in Context (MS COCO) Challenge gewonnen. Das tiefste Netz hat 1202 Schichten.</p>
<p>Materialien:</p>
<ul>
<li>Paper: <a href="http://arxiv.org/abs/1512.03385v1">Deep Residual Learning for Image Recognition</a></li>
<li>Microsoft Blog: <a href="http://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/">Microsoft researchers win ImageNet computer vision challenge</a></li>
</ul>
<h3 id="weitere_1">Weitere</h3>
<ul>
<li>Thoma: <a href="http://arxiv.org/abs/1601.03642">Creativity in Machine Learning</a>, 2016.</li>
<li>Radford, Metz und Chintala: <a href="http://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a>, 2015. Es ist auch <a href="https://github.com/Newmu/dcgan_code">Code online</a> verf&uuml;gbar.</li>
</ul>
<h3 id="blog-artikel">Blog-Artikel</h3>
<ul>
<li>Martin Thoma, 19. Januar 2016: <a href="https://martin-thoma.com/comparing-classifiers/">Comparing Classifiers</a>: Ein kurzer Vergleich verschiedener Klassifikationsalgorithmen auf MNIST und IRIS.</li>
<li>Martin Thoma, 18. Januar 2016: <a href="https://martin-thoma.com/function-approximation/">Function Approximation</a>: Ein sehr kurzes Beispiel, wie man mit gausschen Prozessen Funktionen approximinieren kann.</li>
<li>Zach Dwiel, 15. Januar 2016, <a href="https://github.com/zer0n/deepframeworks/blob/master/README.md">Evaluation of Deep Learning Toolkits</a>: Ein sch&ouml;ner Vergleich zwischen TensorFlow, CNTK, Theano, Torch und Caffe.</li>
<li>Abhinav kumar Gupta, 30. November 2015: <a href="https://www.linkedin.com/pulse/intelligent-photo-ocr-reads-better-than-you-abhinav-kumar-gupta?trk=pulse_spock-articles">Intelligent Photo OCR that reads better than you (Or not)</a></li>
<li>
<ol>
<li>November 2015: <a href="http://www.technologyreview.com/view/543486/single-artificial-neuron-taught-to-recognize-hundreds-of-patterns/?utm_campaign=socialsync&amp;utm_medium=social-post&amp;utm_source=facebook">Single Artificial Neuron Taught to Recognize Hundreds of Patterns</a> (<a href="http://arxiv.org/abs/1511.00083">arxiv</a>)</li>
</ol>
</li>
<li>Mike Schroepfer, 3. November 2015: <a href="https://code.facebook.com/posts/1478523512478471">Teaching machines to see and understand: Advances in AI research</a></li>
<li>Kyle Hill, 22. Juli 2015: <a href="http://nerdist.com/what-happens-when-artificial-intelligence-makes-magic-the-gathering-cards/">What happens when artificial intellicence makes Magic: The Gathering cards</a></li>
<li>Yarin Gal, 3. Juli 2015, <a href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html">What My Deep Model Doesn't Know...</a>: Wie kann man die Unsicherheit eines Modells quantifizieren?</li>
<li>Andrej Karpathy, 21. Mai 2015, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>: Eine Einf&uuml;hrung in RNNs. <strong>Sehr Empfehlenswert</strong>. Eine etwas technischere, aber auch sehr gute Einf&uuml;hrung ist auf <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">colah.github.io</a></li>
<li>Stephanie Yee und Tony Chu: <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">A Visual Introduction to Machine Learning</a></li>
</ul>
<h2 id="software_1">Software</h2>
<!-- e.g. Theano, Keras, ... -->
<ul>
<li>Microsoft ver&ouml;ffentlicht das hauseigene Deep Learning-Toolkit CNTK (<a href="http://blogs.microsoft.com/next/2016/01/25/microsoft-releases-cntk-its-open-source-deep-learning-toolkit-on-github/">Quelle</a>)</li>
<li><a href="https://github.com/nivwusquorum/tensorflow-deepq">Reinforcement Learning using Tensor Flow</a></li>
</ul>
<h2 id="interessante-fragen">Interessante Fragen</h2>
<!-- For example StackExchange -->
<ul>
<li>Neural Networks<ul>
<li><a href="https://groups.google.com/forum/#!topic/lasagne-users/2FgZMACnQR4">How important is ECC for Neural Networks?</a></li>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/42gipr/is_it_only_more_computing_power_why_we_can_now/">Is it only more computing power why we can now train deeper networks?</a></li>
<li><a href="http://datascience.stackexchange.com/q/9672/8820">How exactly does adding a new unit work in Cascade Correlation?</a></li>
<li><a href="http://datascience.stackexchange.com/q/9302/8820">The cross-entropy error function in neural networks</a></li>
<li><a href="http://datascience.stackexchange.com/q/8855/8820">Can the size of a pooling layer be learned?</a></li>
<li><a href="http://datascience.stackexchange.com/q/9233/8820">(Why) do activation functions have to be monotonic?</a></li>
<li><a href="http://datascience.stackexchange.com/q/9175/8820">How do subsequent convolution layers work?</a></li>
</ul>
</li>
<li><a href="http://math.stackexchange.com/q/1626052/6876">What are the limitations of linear regression + feature / label transformation?</a></li>
<li><a href="http://stackoverflow.com/q/34648517/562769">How is a digit recognizer trained when using a Markov Random Field?</a></li>
<li>Nomenclature<ul>
<li><a href="http://cs.stackexchange.com/q/51373/2914">What is the difference between 'features' and 'descriptors' in computer vision / machine learning?</a></li>
<li><a href="http://datascience.stackexchange.com/q/9074/8820">Is there a difference between &ldquo;classification&rdquo; and &ldquo;labeling&rdquo;?</a></li>
<li><a href="http://stackoverflow.com/q/33947823/562769">What is &ldquo;semantic segmentation&rdquo; compared to &ldquo;segmentation&rdquo; and &ldquo;scene labeling&rdquo;?</a></li>
</ul>
</li>
<li><a href="http://cs.stackexchange.com/q/51144/2914">What is the complexity of classification with SVMs?</a></li>
<li><a href="http://datascience.stackexchange.com/q/9172/8820">Can k-means clustering get shells as clusters?</a></li>
<li><a href="http://datascience.stackexchange.com/q/9073/8820">Are all images in ImageNet in the leaves?</a></li>
<li><a href="http://datascience.stackexchange.com/q/10000/8820">What is the difference between a (dynamic) Bayes network and a HMM?</a></li>
</ul>
<h2 id="gemischtes">Gemischtes</h2>
<ul>
<li>Udacity: <a href="https://www.udacity.com/course/deep-learning--ud730">Deep Learning - Taking machine learning to the next level</a>. Ein Deep Learning Kurs von Google.</li>
<li>Quentin de Laroussilhe: <a href="https://docs.google.com/presentation/d/1O6ozzZHHxGzU-McpvEG09hl7K6oQDd2Taw0FOlnxJc8/preview?slide=id.p">Introduction to machine Learning</a>. Eine sehr kurze Einf&uuml;hrung in das maschinelle Lernen.</li>
<li>Daniel Povey: <a href="https://plus.google.com/113952791760990667476/posts/9Hiib9UgUeK">Why simple CNNs with 1x1 kernels may be viewable as learned many-to-many nonlinearities</a>.</li>
<li>Auf <a href="http://www.drivendata.org/">drivendata.org</a> und <a href="http://kaggle.com/">kaggle.com</a> gibt es regelm&auml;&szlig;ig Wettbewerbe.</li>
<li>Auf <a href="http://robotart.org/">robotart.org</a> gibt es f&uuml;r 2016 einen Wettbewerb.</li>
</ul>
<h2 id="klassische-werke">Klassische Werke</h2>
<!-- -->
<p>Alte Werke wieder in Erinnerung rufen und einen Hauch von Nostalgie sp&uuml;ren, oder aber einfach nur ein Gesp&uuml;r daf&uuml;r bekommen, was sich in den letzten Jahren und Jahrzehnten so alles getan hat im Bereich Machine Learning - das soll Sinn und Zweck dieses Abschnitts sein.</p>
<p>Dieses mal zum Thema HMM und deren Anwendung in der Spracherkennung:</p>
<p><a href="http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/">Rabiner, Lawrence R.</a>
<em>"A tutorial on hidden Markov models and selected applications in speech recognition."</em>
Proceedings of the IEEE 77.2 (1989): 257-286. <a href="http://dx.doi.org/10.1109/5.18626">DOI: 10.1109/5.18626</a></p>
<h2 id="interna">Interna</h2>
<!-- About ML-KA itself; can also be a link to posts on this website -->
<h3 id="ag-dank">AG DANK</h3>
<p>In der DANK-Projektgruppe dreht sich alles um Datenanalyse, Data Mining und nat&uuml;rlich - um Machine Learning. Im Oktober 2015, also gleich zum offiziellen Start unserer Hochschulgruppe haben wir diese Untergruppe ins Leben gerufen. Unser erstes Ziel war die Teilnahme bei dem Datenanalyse-Wettbewerb auf der Herbstagung der Arbeitsgruppe Datenanalyse und Numerische Klassifikation (AG DANK) - daher kommt auch der Name. Die Aufgabe war die Analyse von einer Million Autokonfigurationen, die von Nutzern des Online-Autokonfigurators eines gro&szlig;en deutschen Autobauers erstellt wurden.<br/>
Im November 2015 pr&auml;sentierte unser sechsk&ouml;pfiges Team die erarbeiteten Ergebnisse auf der AG DANK Herbsttagung - dabei konnten wir auch gleich unseren ersten Erfolg verbuchen und einen Preis gewinnen.</p>
<p>Unterst&uuml;tzt durch Prof. Geyer-Schulz (KIT) arbeiten wir nun seit Dezember 2015 an einer noch umfangreicheren Analyse des Datensatzes. Zentrale Themen dabei sind Kundensegmentierung, Conjoint-Analyse und das Lernen von Nutzerverhalten.<br/>
In diesem Zusammenhang arbeiten wir an unserem ersten Paper mit dem Titel "Mining consumer-generated product-configuration data", welches wir auf der DAGStat 2016 (14.-18.03.) an der Uni G&ouml;ttingen pr&auml;sentieren werden.</p>
<h3 id="paper-discussion-group">Paper Discussion Group</h3>
<p>Die Paper Discussion Group (PDG) wurde ins Leben gerufen um gemeinsam
wissenschaftliche Ver&ouml;ffentlichungen zu besprechen. Der Gedanke ist, dass man
mehr aus den Ver&ouml;ffentlichungen mitnimmt, wenn man es nicht nur alleine liest,
sondern auch zusammenfasst, anderen erkl&auml;rt und dar&uuml;ber diskutiert.</p>
<p>Bisher wurden folgende Paper besprochen:</p>
<ol>
<li>Stanford: <a href="http://ufldl.stanford.edu/tutorial/">Deep Learning Tutorial</a></li>
<li>Krizhevsky, Sutskever und Hinton: <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>, 2012. ("AlexNet")</li>
<li>Szegedy et al: <a href="http://arxiv.org/abs/1409.4842">Going Deeper with Convolutions</a>, 2014. ("GoogLeNet")</li>
<li>Sermanet et al: <a href="http://arxiv.org/abs/1312.6229">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</a>, 2013.</li>
<li>Nochmals OverFeat</li>
<li>Long, Shelhamer und Darrell: <a href="http://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation</a>, 2014</li>
<li>Olah: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
<li>Mnih, Heess, Graves, Kavukcuoglu: <a href="http://arxiv.org/abs/1406.6247">Recurrent Models of Visual Attention</a></li>
<li>He, Zhang, Ren und Sun: <a href="http://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></li>
</ol>
<p>Mehr Informationen finden sich auf der
<a href="../paper-discussion-group/">Projektseite</a>.</p>
<h2 id="meetings_1">Meetings</h2>
<!-- ML-KA meetings, but not only -->
<ul>
<li>Boston, 12. Mai 2016: Deep Learning Summit (<a href="https://www.re-work.co/events/deep-learning-boston-2016">Link</a>)</li>
<li>M&uuml;nchen, 7. Oktober 2015: Deep Learning in Action #3 (<a href="http://www.meetup.com/de-DE/deeplearning/events/225423302/?eventId=225423302">Link</a>)</li>
</ul>
<!--
<div class="navigation clearfix">
    <div class="alignright">
        <a href="http://ml-ka.de/ml-ruckblick-2/" rel="prev">Nächster Rückblick  »</a>
    </div>
</div>
-->
            </div>
            <!-- /.entry-content -->
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>

<section class="well well-sm">
    <ul class="list-group list-group-flush">
            <li class="list-group-item"><h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
              <ul class="list-group" id="social">
                <li class="list-group-item"><a href="https://www.facebook.com/mlkarlsruhe"><i class="fa fa-facebook-square fa-lg"></i> Facebook</a></li>
              </ul>
            </li>





    <li class="list-group-item"><h4><i class="fa fa-external-link-square fa-lg"></i><span class="icon-label">Links</span></h4>
      <ul class="list-group" id="links">
        <li class="list-group-item">
            <a href="https://github.com/ML-KA/" target="_blank">
                GitHub/ML-KA
            </a>
        </li>
      </ul>
    </li>
    </ul>
</section>
            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2017 Members of the ML-KA group
            &middot; Powered by <a href="https://github.com/DandyDev/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="//ml-ka.de/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="//ml-ka.de/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="//ml-ka.de/theme/js/respond.min.js"></script>


</body>
</html>